# -*- coding: utf-8 -*-
"""senga.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ttGPESNdnVjdpb6G5fFdo9CnaOnfhMJk
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import tensorflow as tf

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split

# Data normalization
from sklearn.preprocessing import StandardScaler

"""connect with google drive to load data"""

from google.colab import drive
drive.mount('/content/drive')

raw_data = pd.read_csv("/content/drive/MyDrive/malware/Malware dataset.csv")
raw_data.head()

raw_data.columns

"""Based on the characteristics of the observations, the dataset was created in a Unix / Lunix-based virtual machine for classification purposes, which are harmless with malware software for Android devices. The data set consists of 100,000 observation data and 35 features. Below is a table of specifications and descriptions."""

# read some statistics of the dataset
raw_data.describe(include="all")

# Check the DataType of our dataset
raw_data.info()

"""The data is already clean."""

#Start Processing
data = raw_data

data["classification"].value_counts()

data['classification'] = data.classification.map({'benign':0, 'malware':1})
data.head()

# Shuffle data
data = data.sample(frac=1).reset_index(drop=True)
data.head()

"""Drawings for ......"""

X = data.drop(["hash","classification",'vm_truncate_count','shared_vm','exec_vm','nvcsw','maj_flt','utime'],axis=1)
Y = data["classification"]

x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=1)

print(x_train)

"""Before we feed to a NN, we need to normalize the data"""

# from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

"""# Data normalization"""

x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
x_test

"""Model creation from tensfrow"""

#Number of attributes
input_size = 27

#Number of Outputs
output_size = 2

# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 50

# define how the model will look like
model = tf.keras.Sequential([
    tf.keras.layers.Dense(hidden_layer_size, input_shape=(input_size,), activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])

model.summary()

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

batch_size = 100
max_epochs = 10
early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)

result = model.fit(x=x_train,
                   y=y_train,
                   batch_size=batch_size,
                   epochs=max_epochs,
                   verbose=1,
                   #callbacks=[early_stopping],
                   validation_split=0.2)

from keras.models import load_model

model.save('/content/drive/MyDrive/malwarenn_model.h5')
model.save('malwarenn_model.h5')

"""# Visualize the result"""

# Visualize the result
acc = result.history['accuracy']
val_acc = result.history['val_accuracy']
loss = result.history['loss']
val_loss = result.history['val_loss']

epochs = range(1, len(acc) + 1)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
sns.set_style("white")
plt.suptitle('Train history', size = 15)

ax1.plot(epochs, acc, "b", label = "Training acc" , color = 'red')
ax1.plot(epochs, val_acc, "b", label = "Validation acc")
ax1.set_title("Training and validation acc")
ax1.legend()

ax2.plot(epochs, loss, "b", label = "Training loss", color = 'red')
ax2.plot(epochs, val_loss, "b", label = "Validation loss", color = 'blue')
ax2.set_title("Training and validation loss")
ax2.legend()

plt.show()

test_loss, test_accuracy = model.evaluate(x_test, y_test)

print('\nTest loss: {0:.6f}. Test accuracy: {1:.6f}%'.format(test_loss, test_accuracy*100.))

test_data=X.tail(2)
test_data=scaler.transform(test_data)
test_data

test_data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/kev/test2.csv")
test_data= test_data.drop(["hash","classification",'vm_truncate_count','shared_vm','exec_vm','nvcsw','maj_flt','utime'],axis=1)
data = data.sample(frac=1).reset_index(drop=True)
data['classification'] = data.classification.map({'benign':0, 'malware':1})
test_data=scaler.transform(test_data)
test_data

import tensorflow as tf
import pandas as pd
import numpy as np

# Assuming you've already trained and saved the model
model = tf.keras.models.load_model('/content/drive/MyDrive/malwarenn_model.h5')


predictions = model.predict(test_data)

# If you want to get the class labels (0 or 1) based on the predictions, you can use argmax to get the class with the highest probability
predicted_classes = np.argmax(predictions, axis=-1)

# np.array(input_df)

pred=["malware" if value == 1 else "benign" for value in predicted_classes]
pred